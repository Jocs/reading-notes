{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "981bf1a5",
   "metadata": {},
   "source": [
    "## 误差反向传播法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d70535a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330.0\n",
      "3.3000000000000003 110.00000000000001 300\n"
     ]
    }
   ],
   "source": [
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x * y\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y\n",
    "        dy = dout * self.x\n",
    "        return dx, dy\n",
    "    \n",
    "apple = 100\n",
    "apple_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "# Forward pass\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)  # 100 * 3\n",
    "total_price = mul_tax_layer.forward(apple_price, tax)  # 300 * 1.1\n",
    "print(total_price)  # 330.0\n",
    "# Backward pass\n",
    "dprice = 1\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)  # dtotal_price\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)  # dapple_price\n",
    "print(dapple, dapple_num, dtax)  # 3.3 110.0 300.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c7d416a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0\n",
      "1 1\n"
     ]
    }
   ],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        return x + y\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        return dx, dy\n",
    "# Example usage of AddLayer\n",
    "add_layer = AddLayer()\n",
    "# Forward pass\n",
    "x = 2.0\n",
    "y = 3.0\n",
    "\n",
    "result = add_layer.forward(x, y)  # 2.0 + 3.0\n",
    "print(result)  # 5.0\n",
    "# Backward pass\n",
    "dout = 1\n",
    "dx, dy = add_layer.backward(dout)  # dresult\n",
    "print(dx, dy)  # 1.0 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6ed779c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 2. 0. 4.]\n",
      "[0. 1. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout.copy()\n",
    "        dx[self.mask] = 0\n",
    "        return dx\n",
    "    \n",
    "# Example usage of ReLU\n",
    "relu_layer = ReLU()\n",
    "# Forward pass\n",
    "x = np.array([-1.0, 2.0, -3.0, 4.0])\n",
    "out = relu_layer.forward(x)  # Apply ReLU\n",
    "print(out)  # [0. 2. 0. 4.]\n",
    "# Backward pass\n",
    "dout = np.array([1.0, 1.0, 1.0, 1.0])  # Gradient from next layer\n",
    "dx = relu_layer.backward(dout)  # Backpropagate through ReLU\n",
    "print(dx)  # [0. 1. 0. 1.]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92fd4211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25       0.19661193 0.19661193]\n"
     ]
    }
   ],
   "source": [
    "class SigmoidLayer:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.out = 1 / (1 + np.exp(-x))\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = dout * (self.out * (1 - self.out))\n",
    "        return dx\n",
    "# Example usage of SigmoidLayer\n",
    "\n",
    "sigmoid_layer = SigmoidLayer()\n",
    "sigmoid_layer.forward(np.array([0.0, 1.0, -1.0]))  # Forward pass\n",
    "# Backward pass\n",
    "dout = np.array([1.0, 1.0, 1.0])  # Gradient from next layer\n",
    "dx = sigmoid_layer.backward(dout)  # Back propagate through Sigmoid\n",
    "print(dx)  # Gradient after Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daf003b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96804405, 0.72819453, 1.06791734])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.rand(2)\n",
    "W  = np.random.rand(2, 3)\n",
    "B = np.random.rand(3)\n",
    "\n",
    "X.shape\n",
    "W.shape\n",
    "B.shape\n",
    "\n",
    "Y = np.dot(X, W) + B\n",
    "\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53a358da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.functions import softmax, cross_entropy_error\n",
    "\n",
    "class AffineLayer:\n",
    "    def __init__(self, W, B):\n",
    "        self.W = W\n",
    "        self.B = B\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.B\n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        return dx\n",
    "    \n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37adeb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # Adds the parent directory to the system path\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # Initialize weights and biases\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "        \n",
    "        # Create layers\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = AffineLayer(self.params['W1'], self.params['b1'])\n",
    "        self.layers['ReLU'] = ReLU()\n",
    "        self.layers['Affine2'] = AffineLayer(self.params['W2'], self.params['b2'])\n",
    "        \n",
    "        # Loss layer\n",
    "        self.loss_layer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.loss_layer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1:\n",
    "            t = np.argmax(t, axis=1)\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        return grads\n",
    "    def gradient(self, x, t):\n",
    "        # Forward pass\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        # Backward pass\n",
    "        dout = 1\n",
    "        dout = self.loss_layer.backward(dout)\n",
    "        \n",
    "        for layer in reversed(self.layers.values()):\n",
    "            dout = layer.backward(dout)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9ea9fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 : 2.0872346495371502e-07\n",
      "b1 : 2.292055180810464e-06\n",
      "W2 : 5.504857720793838e-09\n",
      "b2 : 1.3973923611160232e-07\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # Adds the parent directory to the system path\n",
    "\n",
    "from dataset.mnist import load_mnist\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical= network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))\n",
    "    print(key + \" : \" + str(diff))  # Should be close to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d79b8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 2.3021022751426874, Train Accuracy: 0.07266666666666667, Test Accuracy: 0.0722\n",
      "Iteration 600, Loss: 0.24694378554793744, Train Accuracy: 0.9043, Test Accuracy: 0.9063\n",
      "Iteration 1200, Loss: 0.30523167019327335, Train Accuracy: 0.92445, Test Accuracy: 0.9262\n",
      "Iteration 1800, Loss: 0.19205024498963266, Train Accuracy: 0.9360166666666667, Test Accuracy: 0.9358\n",
      "Iteration 2400, Loss: 0.15361915994946782, Train Accuracy: 0.9445, Test Accuracy: 0.9442\n",
      "Iteration 3000, Loss: 0.1194670700324223, Train Accuracy: 0.9518666666666666, Test Accuracy: 0.9494\n",
      "Iteration 3600, Loss: 0.07608735052061459, Train Accuracy: 0.9559333333333333, Test Accuracy: 0.9544\n",
      "Iteration 4200, Loss: 0.18232933638618884, Train Accuracy: 0.9616833333333333, Test Accuracy: 0.9562\n",
      "Iteration 4800, Loss: 0.05159220218785432, Train Accuracy: 0.9653833333333334, Test Accuracy: 0.9592\n",
      "Iteration 5400, Loss: 0.060470133134077016, Train Accuracy: 0.9685833333333334, Test Accuracy: 0.9616\n",
      "Iteration 6000, Loss: 0.1922570776770629, Train Accuracy: 0.97035, Test Accuracy: 0.9635\n",
      "Iteration 6600, Loss: 0.08989032742536782, Train Accuracy: 0.9731166666666666, Test Accuracy: 0.9645\n",
      "Iteration 7200, Loss: 0.05309208216668251, Train Accuracy: 0.9754166666666667, Test Accuracy: 0.9662\n",
      "Iteration 7800, Loss: 0.051498261833986696, Train Accuracy: 0.9754333333333334, Test Accuracy: 0.965\n",
      "Iteration 8400, Loss: 0.03322898378627818, Train Accuracy: 0.9786333333333334, Test Accuracy: 0.9679\n",
      "Iteration 9000, Loss: 0.04548741876918196, Train Accuracy: 0.9795166666666667, Test Accuracy: 0.9688\n",
      "Iteration 9600, Loss: 0.024111421914518604, Train Accuracy: 0.9807666666666667, Test Accuracy: 0.9703\n"
     ]
    }
   ],
   "source": [
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # Compute gradient\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # Update parameters\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # Record loss\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    # Print progress\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(f\"Iteration {i}, Loss: {loss}, Train Accuracy: {train_acc}, Test Accuracy: {test_acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydata-book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
